{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "138px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "ECMM422-CA1-Classifiers-2020 (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza1122/DL-Assignment/blob/master/ECMM422_CA1_Classifiers_2020_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_tOXQwJ03ef",
        "colab_type": "text"
      },
      "source": [
        "<H1 style=\"text-align: center\">ECMM422 - Machine Learning</H1>\n",
        "<H2 style=\"text-align: center\">Assignment 1: Classifiers</H2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo3ysmZ303eo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "| <font style=\"font-size:larger;\">Date set     | <font style=\"font-size:larger;\">Hand-in date |\n",
        "|:------------------|:-----------------------------------|\n",
        "|<font style=\"font-size:larger;\"> 24th January 2019 |<font style=\"font-size:larger;\">**  12 noon, Wednesday 5th February 2020** |\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjDdYrU103er",
        "colab_type": "text"
      },
      "source": [
        "Candidate number:  <font color=\"red\">*Please put your candidate number here*</font>\n",
        "\n",
        "Marking is anonymous, so please don't write your name or reveal your identity!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDqG2kNK03es",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The module assessment is comprised of your IPython notebooks. There will be four marked assignments, of which this is the first. All assignments will be equally weighted and therefore worth 25% of your module mark. \n",
        "\n",
        "\n",
        "Your IPython notebook should show what you did, what was the\n",
        "result, and what you can conclude from the exercise. For some reports you will need to comment on a paper you have read.   Each report will be\n",
        "assessed on the following criteria:\n",
        "\n",
        "* Does it record what was done in the exercise?\n",
        "* Does it permit the results to be reproduced?\n",
        "* How does the work relate to the theoretical foundations discussed in lectures?\n",
        "* Is it well presented?\n",
        "\n",
        "### Submitting the notebooks\n",
        "\n",
        "Note that an electronic submission is required.  You should also submit the notebook electronically at [electronic hand-in system]( http://empslocal.ex.ac.uk/submit/).  You just need to submit the single .ipynb file that is the notebook, so you should be able to upload it directly from the directory where you have saved it on a University machine or your laptop.  If you're not sure where in your homespace it is, type <code>pwd</code> (print working directory) in a cell to find out where the notebooks are.\n",
        "\n",
        "<b>Note: you must also submit your cover sheet and E-Submit receipt via BART to complete the submission.</b>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t3gljFW03ev",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        " Although you will undoubtedly work collaboratively in the workshops themselves, these are *individual* exercises.  The reports you write should be about the results *you* obtained, and your attention is drawn to the College and University guidelines on collaboration and plagiarism. \n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLf8_PSl03e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3zyJ8ww03e_",
        "colab_type": "text"
      },
      "source": [
        "## 1. Classification\n",
        "\n",
        "In the first part of this workshop, we will work on a simple classification problem with synthetic data. The dataset has one binary target $t\\in \\{0,1\\}$ and two features $\\mathbf{x} = [x_1,x_2]$. This will allow to visualise the data and model decision boundary on 2D plots.\n",
        "\n",
        "First, you should download and load the data in from two files on the VLE, the files should be placed in the same folder as the jupyter notebook. The files contain the $\\mathbf{x}$ and $t$ values for 500 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J4OfBBw03fB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "96c03600-f7c8-4999-d003-aeb518618e3f"
      },
      "source": [
        "# we first load each file in a separate array\n",
        "Xall = np.load('ecmm422_ca1_part1_X.npy')\n",
        "tall = np.load('ecmm422_ca1_part1_t.npy')\n",
        "\n",
        "# then we plot all data using matplotlib - note the indexing to select only samples from one class or the other\n",
        "plt.plot(Xall[tall==0, 0], Xall[tall==0, 1], 'bo')\n",
        "plt.plot(Xall[tall==1, 0], Xall[tall==1, 1], 'ro')\n",
        "plt.axis('scaled')\n",
        "plt.title('All data')\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bde1526dfdd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ecmm422_ca1_part1_X.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ecmm422_ca1_part1_t.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# then we plot all data using matplotlib - note the indexing to select only samples from one class or the other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtall\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtall\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ecmm422_ca1_part1_X.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwEb2z3p03fL",
        "colab_type": "text"
      },
      "source": [
        "It is clear that it cannot be completely separated by a straight line, so these data are said not to be \"linearly separable\".\n",
        "\n",
        "The first step in any machine learning work will be to separate and put aside a test set for final evaluation. Often this set will be sampled randomly from the available data, but for this workshop we will use a specific subset of half the data so that performance is comparable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXWnvKaw03fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I = range(len(tall))    # you could change this for np.random.permutation(len(tall)) to get a random selection\n",
        "                        # do not do this for this workshop as I want to be able to compare final performance. \n",
        "\n",
        "Itrain = I[:len(tall)//2]   # First half of indices for training\n",
        "Itest = I[len(tall)//2:]    # Second half for test\n",
        "\n",
        "Xtr = Xall[Itrain,:] # we define variables for the training set...\n",
        "ttr = tall[Itrain]   \n",
        "\n",
        "Xte = Xall[Itest,:]  # ... and for the test set\n",
        "tte = tall[Itest]   \n",
        "\n",
        "# then we plot both sets\n",
        "plt.subplot(121)\n",
        "plt.plot(Xtr[ttr==0, 0], Xtr[ttr==0, 1], 'bo')\n",
        "plt.plot(Xtr[ttr==1, 0], Xtr[ttr==1, 1], 'ro')\n",
        "plt.axis('scaled')\n",
        "plt.title('Training data')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(Xte[tte==0, 0], Xte[tte==0, 1], 'bo')\n",
        "plt.plot(Xte[tte==1, 0], Xte[tte==1, 1], 'ro')\n",
        "plt.axis('scaled')\n",
        "plt.title('Test data')\n",
        "\n",
        "# note that we would usually also label axes, but in this case the features \n",
        "# are unnamed so this would not add much information. \n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi1-E8-b03fW",
        "colab_type": "text"
      },
      "source": [
        "Good. We can see in those plots that the two distributions are comparable, so we did not create a pathological split (eg, one that would contain only one class!) by accident. \n",
        "\n",
        "<b>NB:</b> We must <b>not</b> use the test data as part of the training process.\n",
        "\n",
        "We will use three different algorithms in this workshop: k-NN, Neural Networks and decision trees. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv9KA4k103fZ",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Classification with k-nearest neighbours\n",
        "As a first step, we will try to model the dataset distribution using k-Nearest Neighbours. \n",
        "We will use the k-nearest neighbour classifier from scikit learn, which is  quite an extensive implementation of various machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8yO76r003fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSHeLYjF03fj",
        "colab_type": "text"
      },
      "source": [
        "To start off, we use **all** the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXMbxPiv03fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 5   # Choose the number of nearest neighbours\n",
        "\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(Xtr, ttr)\n",
        "\n",
        "# we can get the performance on the training set\n",
        "Str = knn.score(Xtr, ttr)\n",
        "\n",
        "# we can also calculate the performance on the test set: \n",
        "Ste = knn.score(Xte, tte)\n",
        "\n",
        "print('Performance with k=%i,\\n\\t- on training set %f \\n\\t- on test set %f' % (k, Str, Ste))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqKkeJv503fu",
        "colab_type": "text"
      },
      "source": [
        "In the following cell, we will set up a grid of points to be classified over the whole domain and then classify them with the k-nn classifier, plotting the result in the corresponding place on the grid.  This gives a nice visualisation of the classfier's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzvglnl803fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we use this to calculate classification accuracy, you can as easily calculate it by hand though. \n",
        "from sklearn import metrics\n",
        "\n",
        "def plot_decision_regions(X, t, model, label='model', show_decision=True, show_probabilities=True):\n",
        "    '''\n",
        "    For convenience, we will use this function to draw the decision boundaries and probabilities of the learnt \n",
        "    models. \n",
        "    Xtr: the features \n",
        "    ttr: the labels\n",
        "    model: the trained model to display\n",
        "    label: the name of the model (for figure captions)\n",
        "    show_decision: draw decision boundary plot (default: true)\n",
        "    show_probabilities: draw class probabilities (default: true)\n",
        "    '''\n",
        "    N, M = 40, 30   # Make these larger to get a smoother picture\n",
        "\n",
        "    X1 = np.linspace(np.min(X[:,0]), np.max(X[:,0]), N)\n",
        "    X2 = np.linspace(np.min(X[:,1]), np.max(X[:,1]), M)\n",
        "    pred = np.zeros((M,N))\n",
        "    prob = np.zeros((M,N,2))\n",
        "\n",
        "    # Writing this double loop is not very efficient, but it is clear.\n",
        "    for nx2, x2 in enumerate(X2):\n",
        "        for nx1, x1 in enumerate(X1):\n",
        "            pred[nx2, nx1] = model.predict([[x1, x2]])          # Predict expects a matrix of features\n",
        "            if show_probabilities:\n",
        "                prob[nx2, nx1, :] = model.predict_proba([[x1, x2]]) # Probabilities of belonging to one class\n",
        "            \n",
        "\n",
        "    if show_decision:\n",
        "        plt.figure() \n",
        "        plt.pcolor(X1, X2, pred, cmap=plt.cm.gray, alpha=0.2)\n",
        "        plt.colorbar()\n",
        "        plt.plot(X[t==0,0], X[t==0,1], 'b.')\n",
        "        plt.plot(X[t==1,0], X[t==1,1], 'r.')\n",
        "        plt.axis('tight')\n",
        "        plt.title('{}: decision'.format(label))\n",
        "\n",
        "    if show_probabilities:\n",
        "        # Plot the probabilites of belonging to the 1 class.\n",
        "        plt.figure()\n",
        "        plt.pcolor(X1, X2, prob[:,:,1], cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "        plt.colorbar()\n",
        "        plt.plot(X[t==0,0], X[t==0,1], 'bo')\n",
        "        plt.plot(X[t==1,0], X[t==1,1], 'ro')\n",
        "        plt.axis('tight')\n",
        "        plt.title('{}: probabilities'.format(label))\n",
        "        \n",
        "plot_decision_regions(Xtr, ttr, model=knn, label='k-NN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqSj3GIN03f3",
        "colab_type": "text"
      },
      "source": [
        "Above we used all the training data and guessed the value of $k$. Much better is to estimate the optimum value of $k$, but dividing the training data into a training and a <b>validation</b> set; the generalisation error is then estimated on the validation set and the $k$ giving the minimum error is used for making predictions about unknown data.\n",
        "\n",
        "Better than just dividing the training data into two is to use $k$ fold <b>cross validation</b> (don't confuse the $k$ in $k$ cross validation with the $k$ in $k$ nearest neighbours!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ombL2xEf03f7",
        "colab_type": "text"
      },
      "source": [
        "The following cell shows how the sklearn routines may be used to produce training and validation sets automatically. More information at <http://scikit-learn.org/stable/modules/cross_validation.html>.  Don't worry about the deprecation warnings!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztbZR8yK03f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Make our 'training data' of 10 examples, each of two columns.\n",
        "# These have just got integers in so that you can easily see which \n",
        "# examples have been selected.\n",
        "X = np.vstack((np.arange(10), np.arange(10))).T + 10\n",
        "t = np.arange(10)+100   # Targets\n",
        "print(\"Features\")\n",
        "print(X)\n",
        "print(\"Targets\")\n",
        "print(t)\n",
        "print()\n",
        "print()\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)    # 5 fold CV here.\n",
        "\n",
        "fold = 0\n",
        "for train, validation in kf.split(X):\n",
        "    print('-------- Fold', fold)\n",
        "    print('Train')\n",
        "    print(X[train])\n",
        "    print(t[train])\n",
        "    print('Test')\n",
        "    print(X[validation])\n",
        "    print(t[validation])\n",
        "    fold += 1\n",
        "    # Notice that each training set consists of 8 of the 10 examples \n",
        "    # and the validation set is the remaining 2.\n",
        "    # You should train the model with X[train] and t[train]\n",
        "    # and estimate the generalisation error on X[validation] and \n",
        "    # t[validation].  Don't forget to average the validation error \n",
        "    # over all the folds - you can also estimate the standard deviation \n",
        "    # to get error bars on the validation error!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXf47AsI03gJ",
        "colab_type": "text"
      },
      "source": [
        "Make a plot of the training and validation errors as $k$ varies from 1 to, say, 50.  Here, just use the number of misclassified samples as the error, but you could also use the cross entropy (how do they differ?). \n",
        "<div style=\"text-align: right\"><b>[10 marks]</b></div>\n",
        "\n",
        "*  What is the best value of $k$?  \n",
        "*  What is the error on the **test** set with the best $k$?\n",
        "*  Make a plot of the decision regions (as above) with the best $k$\n",
        "*  What can you say about the performance of the classifier when $k$ is too large or too small?\n",
        "*  How do you think the optimum $k$ will vary if the amount of training data available is larger or smaller than 250 points?  Can you test your hypothesis?\n",
        "<div style=\"text-align: right\"><b>[10 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIY291ge03gL",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Multi-layer perceptrons (MLP)\n",
        "\n",
        "Multi-layer perceptrons are the sorts of neural networks that have been described in lectures.  In this section we'll use an MLP with weight decay regularisation to classify the same data again. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avbuC1k203gN",
        "colab_type": "text"
      },
      "source": [
        "The next cell sets up an MLP with 5 hidden units and the given regularisation constant ($\\alpha$), trains it a few times from random starting places to find the best minimum (because $E(\\mathbf{w})$ may have local minima) and uses it to classify points on a grid as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTi6JmGT03gP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the Multi-Layer Perceptron class from Scikit-Learn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# because initialisation is random, we train 10 times and take the best model (according to *training* error)\n",
        "Ebest = np.Inf\n",
        "for n in range(10):\n",
        "    mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,))\n",
        "    mlp.fit(Xtr, ttr)\n",
        "    if mlp.loss_ < Ebest:\n",
        "        mlpbest = mlp\n",
        "        Ebest = mlp.loss_\n",
        "mlp = mlpbest\n",
        "\n",
        "\n",
        "plot_decision_regions(X=Xtr, t=ttr, model=mlp, label='MLP', show_decision=True, show_probabilities=True)\n",
        "\n",
        "Ste = mlp.score(Xte, tte)\n",
        "Str = mlp.score(Xtr, ttr)\n",
        "print('Performance with k=%i,\\n\\t- on training set %f \\n\\t- on test set %f' % (k, Str, Ste))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c225DkeG03gV",
        "colab_type": "text"
      },
      "source": [
        "Experiment with different values of the regularisation constant, plotting the decision boundaries to visualise how the regularisation forces a smooth or more wiggly boundary.  Remember to vary $\\alpha$ on a logarithmic scale and it could range between $10^{-16}$ (effectively 0) and $1$. \n",
        "\n",
        "<div style=\"text-align: right\"><b>[5 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ICZQK7t03gb",
        "colab_type": "text"
      },
      "source": [
        "Use cross-validation to choose the best value of the regularisation constant for these data.   Plot the training and validation data errors versus $\\alpha$.  How well does the validation error correspond to the actual test error?\n",
        "\n",
        "<div style=\"text-align: right\"><b>[5 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS8ifqDf03gd",
        "colab_type": "text"
      },
      "source": [
        "Bonus question:  How should the best value of $\\alpha$ vary as the number of training examples is increased? Why? Can you test it? \n",
        "\n",
        "<div style=\"text-align: right\"><b>[5 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uroJtkRU03ge",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Support Vector Machines (SVM)\n",
        "\n",
        "Support vector machines (SVMs) are popular and powerful classifiers.  We'll deal with them in detail later in the module.  In the following you will use SVMs with both a \"linear kernel\" and with a RBF kernel.  With a linear kernel the SVM can only separate the classes with a straight line or a (hyper)plane, but an RBF kernel is nonlinear and allows the SVM to generate curved decision boundaries.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syfy9hGd03gg",
        "colab_type": "text"
      },
      "source": [
        "We'll use the support vector machines in scikit learn.  Here's an example of training a SVM using a linear kernel (i.e., no non-linearity)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT2T39C003gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel='linear', probability=True)    # SVM with a linear kernel, \n",
        "                                                # note that enabling probabilities is slower\n",
        "svm.fit(Xtr, ttr)                     # Train it on the training data\n",
        "\n",
        "plot_decision_regions(X=Xtr, t=ttr, model=svm, label='Linear SVM', show_decision=True, show_probabilities=True)\n",
        "\n",
        "Ste = svm.score(Xte, tte)\n",
        "Str = svm.score(Xtr, ttr)\n",
        "print('Performance:\\n\\t- on training set %f \\n\\t- on test set %f' % (Str, Ste))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LOJkTHQ03gt",
        "colab_type": "text"
      },
      "source": [
        "Now use the SVM with <code>kernel='rbf'</code> to classify the data, but with a non-linear mapping so that the decision boundary can be non-linear. \n",
        "\n",
        "In this case the SVM has two additional parameters $C > 0$ and $\\gamma$.  $C$ is a regularisation constant that controls how strongly points that lie on the wrong side of the decision boundary are penalised.  When $C$ is large, they are heavily penalised which results in the boundary being flexible, but when it is small they are lightly penalised so the boundary tends to be straighter.  You can set the value of $C$ (and $\\gamma$) with\n",
        "\n",
        "        svm = SVC(kernel='rbf', C=1, gamma=0.1) \n",
        "\n",
        "The default value of $C$ is 1 and the default value of $\\gamma$ is 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hbr8r1t03gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel='rbf', C=1, gamma=0.1, probability=True)            # SVM with a RBF kernel\n",
        "svm.fit(Xtr, ttr)                     # Train it on the training data\n",
        "\n",
        "plot_decision_regions(X=Xtr, t=ttr, model=svm, label='Linear SVM', show_decision=True, show_probabilities=True)\n",
        "\n",
        "Ste = svm.score(Xte, tte)\n",
        "Str = svm.score(Xtr, ttr)\n",
        "print('Performance with C=%f, gamma=%f,\\n\\t- on training set %f \\n\\t- on test set %f' % (1, 0.1, Str, Ste))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlHDmHxJ03g2",
        "colab_type": "text"
      },
      "source": [
        "The $\\gamma$ parameter controls the width of the RBF kernel, that is how close training samples must be to interact with each other.  When $\\gamma$ is large the training samples must be close to interact and the decision boundary tends to be nonlinear, whereas when $\\gamma$ is small it tends to be smooth.\n",
        "\n",
        "Use cross validation on the *training* data to choose the best value of $C$ and $\\gamma$.  Try a few values of $C$ and $\\gamma$ by hand first of all to find appropriate ranges and it will  be best to vary them on a logrithmic scale.  You may care to investigate and use the sklearn helper functions for searching a grid of values for the best cross validation value: see <http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#example-svm-plot-rbf-parameters-py> or write your own!\n",
        "\n",
        "What is the resulting accuracy on the test data and the best decision boundary?\n",
        "\n",
        "<div style=\"text-align: right\"><b>[15 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yp3hXq303g3",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Digits dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J77veSOI03g8",
        "colab_type": "text"
      },
      "source": [
        "In the second part, we will experiment with the same algorithm on a more complex dataset: examples images of hand-written digits. For this we will use a standard dataset provided by sklearn: <i>digits</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zES3dPH03g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# (note: we overwrite the previous data here, make sure to reload it if you go back to part 1)\n",
        "Xall, tall = datasets.load_digits(return_X_y=True)\n",
        "\n",
        "print(Xall.shape)\n",
        "print(np.unique(tall))\n",
        "\n",
        "# print some examples of data from the dataset\n",
        "for j in range(10):\n",
        "    plt.subplot(2,5,j+1)\n",
        "    plt.imshow(np.reshape(Xall[j,:], (8,8)), cmap=plt.cm.gray_r)\n",
        "    plt.title(\"%d\" % tall[j])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVOh0gVX03hF",
        "colab_type": "text"
      },
      "source": [
        "As you can see there are 1797 images arranged as 64-dimensional vectors, and defining 10 classes. In this case, the observed variables have many more dimensions, but we can use <i>multidimensional scaling</i> to visualise the distribution of the classes somehow (note that MDS on a large dataset can take a while).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQQj0kPY03hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# we select a subset of samples for speed \n",
        "Ntrain=1000\n",
        "J = np.random.permutation(Xall.shape[0])[:Ntrain]\n",
        "X = Xall[J,:]\n",
        "t = tall[J]   # Features\n",
        "\n",
        "# we then optimise the multidimensional scaling \n",
        "tsne = TSNE()\n",
        "Xe = tsne.fit_transform(X)\n",
        "\n",
        "# we then draw the classes on a 2D plot\n",
        "for i in range(10):\n",
        "    plt.plot(Xe[t==i,0], Xe[t==i,1], 'x', label='{}'.format(i))\n",
        "plt.legend(loc='right')\n",
        "plt.title('Digits dataset (t-SNE plot)')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwCVjAR03hW",
        "colab_type": "text"
      },
      "source": [
        "This means that you have problem of <b>multiclass</b> classification. Some algorithms such as k-NN or MLPs can handle multiclass, whereas some other such as SVM need some additional work. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXI2ObmM03hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Example with k-NN:  \n",
        "\n",
        "I = range(len(tall))    # you could change this for np.random.permutation(len(tall)) to get a random selection\n",
        "                        # do not do this for this workshop as I want to be able to compare final performance. \n",
        "\n",
        "Itrain = I[:len(tall)//2]   # First half of indices for training\n",
        "Itest = I[len(tall)//2:]    # Second half for test\n",
        "\n",
        "Xtr = Xall[Itrain,:] # we define variables for the training set...\n",
        "ttr = tall[Itrain]   \n",
        "\n",
        "Xte = Xall[Itest,:]  # ... and for the test set\n",
        "tte = tall[Itest]   \n",
        "\n",
        "k = 5   # Choose the number of nearest neighbours\n",
        "\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(Xtr, ttr)\n",
        "\n",
        "# we can get the performance on the training set\n",
        "Str = knn.score(Xtr, ttr)\n",
        "\n",
        "# we can also calculate the performance on the test set: \n",
        "Ste = knn.score(Xte, tte)\n",
        "\n",
        "print('Performance with k=%i,\\n\\t- on training set %f \\n\\t- on test set %f' % (k, Str, Ste))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDpVFEX03hg",
        "colab_type": "text"
      },
      "source": [
        "In a multiclass setting it is often worth calculating another error measure: the confusion matrix. This matrix tells you where your error comes from and whether two classes are specially confused.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMx6rE_t03hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "import itertools\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# we predict the classes on the test data\n",
        "pte = knn.predict(Xte)\n",
        "\n",
        "# and we calculate the confusion matrix\n",
        "cm = confusion_matrix(tte, pte)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "# code from: https://scikit-learn.org/0.18/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "# now included in sklearn 0.22\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "plot_confusion_matrix(cm, np.unique(tte))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dESPQLE303ht",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Digits with knn \n",
        "As a first shot, we will use the k-NN algorithm to try and model this dataset.\n",
        "As we did before, use k-fold cross validation to find the best value of $k$. \n",
        "<div style=\"text-align: right\"><b>[5 marks]</b></div>\n",
        "\n",
        "Calculate the confusion matrix for the best parameter, show some examples of the misclassifications and try to explain the remaining errors. \n",
        "<div style=\"text-align: right\"><b>[5 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUrfBZo603hv",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Digits with MLP\n",
        "As a second attempt \n",
        "As we did before, use k-fold cross validation to train an MLP on this data, optimising regularisation parameter $\\alpha$ and the number of hidden units. \n",
        "<div style=\"text-align: right\"><b>[10 marks]</b></div>\n",
        "\n",
        "Calculate the confusion matrix for the best parameter, show some examples of the misclassifications and try to explain the remaining errors. \n",
        "<div style=\"text-align: right\"><b>[5 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuSudDFg03hw",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Digits with SVM \n",
        "The last exercise in this workshop will be to use the SVM classifier for this data. In standard, SVMs only allow for binary classification. The standard approach to use them on multiclass problem is to decompose the problem in N 1 vs all classification problem: ie, in this case, this would mean learning one SVM for classifying digit 0 against all other (eg, [1,2,3,4,5,6,7,8,9]), a second classifier for classifying digit 1 against all others, etc. \n",
        "The final classification is given by the classifier that yields the strongest confidence. \n",
        "\n",
        "This is not too hard to implement, but luckily sklearn implements 1 vs all as standard in the SVC classes. \n",
        "\n",
        "Train linear an non linear SVMs, using k-fold cross validation to find the best value for $C$ and $\\gamma$. \n",
        "<div style=\"text-align: right\"><b>[10 marks]</b></div>\n",
        "\n",
        "Calculate the confusion matrix for the best parameter, show some examples of the misclassifications and try to explain the remaining errors. \n",
        "<div style=\"text-align: right\"><b>[5 marks]</b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e012E0_S03hy",
        "colab_type": "text"
      },
      "source": [
        "Compare performance of all 3 algorithms for this problem. Discuss the differences in the remaining errors and what would be your suggestion going forward with this dataset. \n",
        "<div style=\"text-align: right\"><b>[10 marks]</b></div>"
      ]
    }
  ]
}